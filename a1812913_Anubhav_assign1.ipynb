{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## Assignment 1 \n",
    "### Anubhav a1812913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "### 1. Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import string\n",
    "# nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords #Importing Stop Words\n",
    "#Tokenize the words\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize #to TOkenize the words\n",
    "\n",
    "\n",
    "#reading data\n",
    "train_text = []\n",
    "with open('reviews_train.txt') as f:\n",
    "    train_text = f.readlines()\n",
    "# train_text\n",
    "\n",
    "# First convert to lowercase the train data\n",
    "train_text_lower = [word.lower() for word in train_text ]\n",
    "\n",
    "# Second correct the punctuation of training data\n",
    "filter_train_text = [i.translate(i.maketrans('', '', string.punctuation)) for i in train_text_lower]\n",
    "\n",
    "#to Tokenize the words\n",
    "token_words_train = [word_tokenize(text) for text in filter_train_text]\n",
    "token_words_train\n",
    "\n",
    "# #Removing Stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# Fiter_tokens_Test =[]\n",
    "# for line in token_words_Test:\n",
    "#     fltr_line = [word for word in line if word not in stop_words]\n",
    "#     Fiter_tokens_Test.append(list(fltr_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39775241",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e4842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# limit to the first 1000 articles\n",
    "# num_rows = 10000\n",
    "num_rows = len(newsgroups.data) # full size\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "train_data = newsgroups.data[:int(num_rows*0.8)]\n",
    "train_target = newsgroups.target[:int(num_rows*0.8)]\n",
    "test_data = newsgroups.data[int(num_rows*0.8):]\n",
    "test_target = newsgroups.target[int(num_rows*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b38a1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d286790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333cefa8",
   "metadata": {},
   "source": [
    "### 3. Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292820d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Sentiment classification using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c816a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333ba5cb",
   "metadata": {},
   "source": [
    "### 5. Sentiment classification using VADER sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea3a2",
   "metadata": {},
   "source": [
    "### 6. (optional challenge 3 points) Combine VADER with the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf2087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d3d17f",
   "metadata": {},
   "source": [
    "### 7. References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
