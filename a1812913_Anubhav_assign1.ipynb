{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6730fecf",
      "metadata": {
        "id": "6730fecf"
      },
      "source": [
        "## Assignment 1 \n",
        "### Anubhav a1812913"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All Imports"
      ],
      "metadata": {
        "id": "zegVDun6EcOi"
      },
      "id": "zegVDun6EcOi"
    },
    {
      "cell_type": "code",
      "source": [
        "#import Libraries\n",
        "import string\n",
        "nltk.download('stopwords')  \n",
        "from nltk.corpus import stopwords #Importing Stop Words\n",
        "#Tokenize the words\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize #to TOkenize the words\n",
        "# import lemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYKtbHSFEnoT",
        "outputId": "c636d6df-4a66-40b9-be14-1a2c9a934a9a"
      },
      "id": "xYKtbHSFEnoT",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df2aac6",
      "metadata": {
        "id": "3df2aac6"
      },
      "source": [
        "### 1. Reading datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "e967e07b",
      "metadata": {
        "id": "e967e07b"
      },
      "outputs": [],
      "source": [
        "#Reading data\n",
        "train_text = []\n",
        "with open('drive/My Drive/NLP/Ass1/Data/reviews_train.txt', encoding='latin-1') as f:\n",
        "    train_text = f.readlines()\n",
        "# train_text\n",
        "\n",
        "#reading data for testing \n",
        "test_text = []\n",
        "with open('drive/My Drive/NLP/Ass1/Data/reviews_test.txt', encoding='latin-1') as f:\n",
        "    test_text = f.readlines()\n",
        "# test_text\n",
        "\n",
        "\n",
        "#to Tokenize the words\n",
        "token_words_train = [word_tokenize(text) for text in train_text]\n",
        "token_words_test = [word_tokenize(text) for text in test_text]\n",
        "token_words_train\n",
        "token_words_test\n",
        "\n",
        "\n",
        "#Seperating words into labels and training data\n",
        "train_data = [i[1:-1] for i in token_words_train]\n",
        "temp_train_labels = [i[0] for i in token_words_train]\n",
        "train_labels = [word.replace('__label__1', 'negative').replace('__label__2', 'positive') for word in temp_train_labels]\n",
        "test_data = [i[1:-1] for i in token_words_test]\n",
        "temp_test_labels = [i[0] for i in token_words_test]\n",
        "test_labels = [word.replace('__label__1', 'negative').replace('__label__2', 'positive') for word in temp_test_labels]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d75afc",
      "metadata": {
        "id": "31d75afc"
      },
      "source": [
        "### 2. Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "rm_stp_train  = [[word for word in line if word.lower() not in stop_words]for line in train_data]\n",
        "rm_stp_test = [[word for word in line if word.lower() not in stop_words]for line in test_data]\n",
        "\n",
        "#Remove punctuation and all non-alphanumaric characters\n",
        "# rm_punct_train = [[i.translate(i.maketrans('', '', string.punctuation)) for i in line ] for line in rm_stp_train]\n",
        "# rm_punct_test = [[i.translate(i.maketrans('', '', string.punctuation)) for i in line] for line in rm_stp_test]\n",
        "rm_punct_train = [[word for word in line if word.isalnum()] for line in rm_stp_train]\n",
        "rm_punct_test = [[word for word in line if word.isalnum()] for line in rm_stp_test]\n",
        "\n",
        "#Creating Version 1\n",
        "train_version1 = rm_punct_train\n",
        "test_version1 = rm_punct_test\n",
        "\n",
        "#Performing lowercasing of words on version1 for Version 2\n",
        "train_version2 = [[word.lower() for word in line]for line in train_version1]\n",
        "test_version2 = [[word.lower() for word in line]for line in test_version1]\n",
        "\n",
        "#Performing Lemmatization of words on version2 for Version 3\n",
        "\n",
        "train_version3 = [[lemmatizer.lemmatize(word) for word in line]for line in train_version2]\n",
        "test_version3 = [[lemmatizer.lemmatize(word) for word in line]for line in test_version2]"
      ],
      "metadata": {
        "id": "mUF1RHqLJTKJ"
      },
      "id": "mUF1RHqLJTKJ",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing Shapes of both Versions\n",
        "print(f\"Shape of Version 1 Training : {len(train_version1)}\\n\")\n",
        "print(f\"Shape of Version 1 Testing : {len(test_version1)}\\n\")\n",
        "print(f\"Shape of Version 2 Training : {len(train_version2)}\\n\")\n",
        "print(f\"Shape of Version 2 Testing : {len(test_version2)}\\n\")\n",
        "print(f\"Shape of Version 3 Training : {len(train_version3)}\\n\")\n",
        "print(f\"Shape of Version 3 Testing : {len(test_version3)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQXHfXJALUoS",
        "outputId": "c511cd29-25f8-4e53-dbe8-dfa11c9d7c14"
      },
      "id": "ZQXHfXJALUoS",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Version 1 Training : 300000\n",
            "\n",
            "Shape of Version 1 Testing : 100000\n",
            "\n",
            "Shape of Version 2 Training : 300000\n",
            "\n",
            "Shape of Version 2 Testing : 100000\n",
            "\n",
            "Shape of Version 3 Training : 300000\n",
            "\n",
            "Shape of Version 3 Testing : 100000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d286790",
      "metadata": {
        "id": "2d286790"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "333cefa8",
      "metadata": {
        "id": "333cefa8"
      },
      "source": [
        "### 3. Dataset analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform basic dataset analysis: proportion of positive and negative sentiments, number of \n",
        "unique words in all three versions. Present the results in charts or tables. Discuss the results \n",
        "and how this may affect classification performance and the selection of performance \n",
        "metrics. "
      ],
      "metadata": {
        "id": "0v1BG5FfS3ql"
      },
      "id": "0v1BG5FfS3ql"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c292820d",
      "metadata": {
        "id": "c292820d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "067cebcf",
      "metadata": {
        "id": "067cebcf"
      },
      "source": [
        "### 4. Sentiment classification using Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use at least two machine learning methods of your choice to classify the three versions \n",
        "obtained in point 2 into positive or negative polarity. Show and compare test results of this \n",
        "experiment. Choose the best dataset version and model and show the test results on this \n",
        "model. Discuss the results in terms of under/overfit. "
      ],
      "metadata": {
        "id": "nMJnRbEdWAlf"
      },
      "id": "nMJnRbEdWAlf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c816a9",
      "metadata": {
        "id": "23c816a9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "333ba5cb",
      "metadata": {
        "id": "333ba5cb"
      },
      "source": [
        "### 5. Sentiment classification using VADER sentiment lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use VADER sentiment lexicon/utility to classify the test set reviews into the polarity. \n",
        "Compare results with those obtained in point 4. Discuss the differences."
      ],
      "metadata": {
        "id": "wt6VqN5cWB70"
      },
      "id": "wt6VqN5cWB70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a6ebe4",
      "metadata": {
        "id": "58a6ebe4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ba7ea3a2",
      "metadata": {
        "id": "ba7ea3a2"
      },
      "source": [
        "### 6. (optional challenge 3 points) Combine VADER with the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(optional challenge up to 10/100 points) Combine VADER sentiment analysis output with the \n",
        "classification model. Classify best_data and compare with results with point 4"
      ],
      "metadata": {
        "id": "6KkdgNvIWHf0"
      },
      "id": "6KkdgNvIWHf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99cf2087",
      "metadata": {
        "id": "99cf2087"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "23d3d17f",
      "metadata": {
        "id": "23d3d17f"
      },
      "source": [
        "### 7. References"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}